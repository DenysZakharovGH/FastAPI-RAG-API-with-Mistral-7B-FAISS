# ðŸš€ FastAPI RAG API with Mistral-7B & FAISS

A **production-ready Retrieval-Augmented Generation (RAG) API** built with **FastAPI**, **FAISS**, and **Mistral-7B-Instruct**.

This project demonstrates how to build a **controlled, async-safe LLM backend** that answers questions **only from your own data**, minimizing hallucinations while keeping latency and costs under control.

---

## âœ¨ Features

* âš¡ **FastAPI (ASGI)** â€” high-performance async web framework
* ðŸ§  **Mistral-7B-Instruct** â€” powerful open-weight LLM
* ðŸ” **FAISS Vector Database** â€” fast cosine similarity search
* ðŸ§© **Complete RAG pipeline**

  * ingestion
  * chunking
  * embeddings
  * retrieval
  * generation
* ðŸ§µ **Async-safe LLM execution** using `run_in_threadpool`
* ðŸ“¦ **Metadata support** (source, document info per chunk)
* ðŸ›¡ï¸ **Hallucination control** (answers only from retrieved context)
* ðŸ“ **Token-aware prompt construction**
* ðŸ”„ **Background tasks** for logging
* ðŸ³ **Docker-ready architecture**

---

## ðŸ—ï¸ Architecture

```
Client
  â†“
FastAPI (async)
  â†“
Pydantic Validation
  â†“
FAISS Similarity Search
  â†“
RAG Prompt Assembly
  â†“
Mistral-7B-Instruct
  â†“
Answer + Sources
```

---

## ðŸ”§ Requirements

* Python **3.10+**
* GPU recommended (**â‰¥ 8GB VRAM**)
* Linux / Windows / macOS

### Python dependencies

```bash
pip install -r requirements.txt
```

Main libraries:

* `fastapi`
* `uvicorn`
* `torch`
* `transformers`
* `faiss-cpu` or `faiss-gpu`
* `sentence-transformers`

---

## â–¶ï¸ Running the API

```bash
uvicorn app.main:app --reload
```

Open API docs:

ðŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

## ðŸ“¤ Example Request

```http
POST /ask
Content-Type: application/json

{
  "question": "Length of a giraffe tongue?"
}
```

### ðŸ“¥ Example Response

```json
{
  "answer": "The length of a giraffe's tongue can measure between 18 to 20 inches.",
  "sources": [
    "The giraffes tongue is about 18 inches long",
    "giraffes have a 17 inch long tongue",
    "A giraffes tongue can measure 20 inches in length"
  ]
}
```
---

[![Demo](docs/demo.gif)]

---

## ðŸ§  RAG Pipeline Explained

### 1ï¸âƒ£ Ingestion

Load raw documents and store their source information.

### 2ï¸âƒ£ Chunking

Split documents into **300â€“500 token chunks** with overlap.

### 3ï¸âƒ£ Embeddings

Convert text chunks into vectors using a sentence embedding model.

### 4ï¸âƒ£ Vector Database

Store embeddings in **FAISS** for fast similarity search.

### 5ï¸âƒ£ Retrieval

Search for **top-k most similar chunks** using cosine similarity.

### 6ï¸âƒ£ Prompt Assembly

Inject retrieved context into an instruction-based prompt.

### 7ï¸âƒ£ Generation

Generate an answer using **Mistral-7B-Instruct**.

---

## ðŸ›¡ï¸ Hallucination Control

The model is explicitly instructed:

> *If the answer is not in the provided context, say "I don't know".*

Additional safety:

* limited context window
* source-aware responses
* optional similarity confidence threshold

---

## ðŸ§µ Async-Safe LLM Execution

LLM inference is **blocking**, so it is executed safely using:

```python
await run_in_threadpool(generate_answer, prompt)
```

This prevents blocking the FastAPI event loop and allows concurrent requests.

---

## ðŸ³ Docker (Optional)

```bash
docker build -t rag-api .
docker run -p 8000:8000 rag-api
```

Docker Compose support can be added for:

* GPU
* Redis cache
* external vector stores

---

## ðŸ“Œ Use Cases

* Internal knowledge assistants
* Document Q&A systems
* RAG-powered chatbots
* AI search APIs
* LLM experimentation with full data control

---

## ðŸ”® Roadmap

* âš¡ Streaming responses (token-by-token)
* ðŸ§µ Concurrency limiter (Semaphore)
* ðŸ›¡ï¸ Hallucination confidence scoring
* ðŸ’¾ Redis caching
* ðŸ³ GPU-optimized Docker Compose

---

## ðŸ§  Philosophy

> **LLMs should answer only what your data allows them to answer.**

This project focuses on **correctness, observability, and control** rather than raw generation.

---

## ðŸ“„ License

MIT License

